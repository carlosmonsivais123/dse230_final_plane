{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Process in GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to run this process end to end we have to run it in GCP using DataProc to spin up clusters and run our PySpark code there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run the process, just run the cell below which will run the main.py file kicking off the other Python files that have dependencies on this file.\n",
    "### The logs of the process will be shown below. More information on the files can be read in the README.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataProc_Files/dataproc_spark_session.py has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/DataProc_Files/dataproc_spark_session.py\n",
      "DataProc_Files/eda_dataproc_main.py has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/DataProc_Files/eda_dataproc_main.py\n",
      "DataProc_Files/eda_dataproc_spark_commands.py has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/DataProc_Files/eda_dataproc_spark_commands.py\n",
      "DataProc_Files/features_dataproc_main.py has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/DataProc_Files/features_dataproc_main.py\n",
      "DataProc_Files/features_dataproc_spark_commands.py has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/DataProc_Files/features_dataproc_spark_commands.py\n",
      "DataProc_Files/model_dataproc_main.py has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/DataProc_Files/model_dataproc_main.py\n",
      "DataProc_Files/model_dataproc_spark_commands.py has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/DataProc_Files/model_dataproc_spark_commands.py\n",
      "\n",
      "EDA: Creating Pyspark Cluster in Dataproc\n",
      "Cluster created successfully: dse-230-pyspark\n",
      "\n",
      "Cluster has been created\n",
      "\n",
      "EDA: Running PySpark Code.\n",
      "Job finished successfully: b\"22/05/30 19:42:09 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\\n22/05/30 19:42:09 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\\n22/05/30 19:42:10 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\\n22/05/30 19:42:10 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\\n22/05/30 19:42:10 INFO org.sparkproject.jetty.util.log: Logging initialized @5578ms to org.sparkproject.jetty.util.log.Slf4jLog\\n22/05/30 19:42:10 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09\\n22/05/30 19:42:10 INFO org.sparkproject.jetty.server.Server: Started @5691ms\\n22/05/30 19:42:10 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@1f91c4ca{HTTP/1.1, (http/1.1)}{0.0.0.0:43697}\\n22/05/30 19:42:11 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dse-230-pyspark-m/10.128.15.206:8032\\n22/05/30 19:42:11 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dse-230-pyspark-m/10.128.15.206:10200\\n22/05/30 19:42:12 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\\n22/05/30 19:42:12 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\\n22/05/30 19:42:13 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1653939655347_0001\\n22/05/30 19:42:14 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dse-230-pyspark-m/10.128.15.206:8030\\n22/05/30 19:42:16 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\\n22/05/30 19:42:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://plane-pyspark-run/Spark_Data_Output/origin_airport_count.csv/' directory.\\n22/05/30 19:42:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://plane-pyspark-run/Spark_Data_Output/destination_airport_count.csv/' directory.\\n22/05/30 19:43:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://plane-pyspark-run/Spark_Data_Output/corr_matrix.csv/' directory.\\n22/05/30 19:43:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://plane-pyspark-run/Spark_Data_Output/pairplot.csv/' directory.\\n22/05/30 19:45:25 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\\n22/05/30 19:45:27 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://plane-pyspark-run/Spark_Data_Output/summary_table.csv/' directory.\\n22/05/30 19:46:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://plane-pyspark-run/Spark_Data_Output/null_value_counts.csv/' directory.\\n22/05/30 19:46:00 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@1f91c4ca{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\\n\"\n",
      "\n",
      "\n",
      "EDA: Deleting PySpark Cluster.\n",
      "Cluster dse-230-pyspark successfully deleted.\n",
      "\n",
      "EDA: Creating EDA Plots and sending them to GCP\n",
      "EDA_Static_Images/Origin_Destination_Airport_Count.png has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/EDA_Static_Images/Origin_Destination_Airport_Count.png\n",
      "EDA_Static_Images/Correlation_Plot.png has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/EDA_Static_Images/Correlation_Plot.png\n",
      "EDA_Static_Images/Summary_Table.png has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/EDA_Static_Images/Summary_Table.png\n",
      "EDA_Static_Images/Null_Values.png has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/EDA_Static_Images/Null_Values.png\n",
      "EDA_Static_Images/Pairplot.png has been uplaoded succesfully at https://storage.googleapis.com/plane-pyspark-run/EDA_Static_Images/Pairplot.png\n",
      "\n",
      "Feature Engineering: Creating Pyspark Cluster in Dataproc\n",
      "Cluster created successfully: dse-230-pyspark\n",
      "\n",
      "Cluster has been created\n",
      "\n",
      "Feature Engineering: Running PySpark Code.\n",
      "Job finished successfully: b\"22/05/30 19:51:02 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\\n22/05/30 19:51:02 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\\n22/05/30 19:51:02 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\\n22/05/30 19:51:02 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\\n22/05/30 19:51:02 INFO org.sparkproject.jetty.util.log: Logging initialized @5121ms to org.sparkproject.jetty.util.log.Slf4jLog\\n22/05/30 19:51:02 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09\\n22/05/30 19:51:02 INFO org.sparkproject.jetty.server.Server: Started @5232ms\\n22/05/30 19:51:02 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@94651f{HTTP/1.1, (http/1.1)}{0.0.0.0:35057}\\n22/05/30 19:51:03 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dse-230-pyspark-m/10.128.15.210:8032\\n22/05/30 19:51:03 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dse-230-pyspark-m/10.128.15.210:10200\\n22/05/30 19:51:04 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\\n22/05/30 19:51:04 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\\n22/05/30 19:51:05 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1653940158038_0001\\n22/05/30 19:51:06 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dse-230-pyspark-m/10.128.15.210:8030\\n22/05/30 19:51:09 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\\n22/05/30 19:51:28 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\\n22/05/30 19:53:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://plane-pyspark-run/Spark_Data_Output/model_df.csv/' directory.\\n22/05/30 19:53:34 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@94651f{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\\n\"\n",
      "\n",
      "\n",
      "EDA: Deleting PySpark Cluster.\n",
      "Cluster dse-230-pyspark successfully deleted.\n",
      "\n",
      "Model: Creating Pyspark Cluster in Dataproc\n",
      "Cluster created successfully: dse-230-pyspark\n",
      "\n",
      "Cluster has been created\n",
      "\n",
      "Model: Running PySpark Code.\n",
      "Job finished successfully: b\"22/05/30 19:59:04 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\\n22/05/30 19:59:04 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\\n22/05/30 19:59:05 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\\n22/05/30 19:59:05 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\\n22/05/30 19:59:05 INFO org.sparkproject.jetty.util.log: Logging initialized @6323ms to org.sparkproject.jetty.util.log.Slf4jLog\\n22/05/30 19:59:05 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09\\n22/05/30 19:59:05 INFO org.sparkproject.jetty.server.Server: Started @6469ms\\n22/05/30 19:59:05 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@5e961ee6{HTTP/1.1, (http/1.1)}{0.0.0.0:36317}\\n22/05/30 19:59:06 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dse-230-pyspark-m/10.128.15.208:8032\\n22/05/30 19:59:06 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at dse-230-pyspark-m/10.128.15.208:10200\\n22/05/30 19:59:07 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\\n22/05/30 19:59:07 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\\n22/05/30 19:59:09 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1653940641891_0001\\n22/05/30 19:59:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dse-230-pyspark-m/10.128.15.208:8030\\n22/05/30 19:59:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\\nSpark_Data_Output/model_df.csv/part-00000-a9bae037-ad6f-4fbc-aa40-7ada7438ece2-c000.csv\\n22/05/30 20:00:35 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\\n22/05/30 20:06:09 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\\n22/05/30 20:06:09 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\\n22/05/30 20:06:12 INFO breeze.optimize.StrongWolfeLineSearch: Line search t: 1.5391243963160504 fval: 1.5995231994671832 rhs: 1.6317912102300123 cdd: -0.0012665233102849153\\n22/05/30 20:06:12 INFO breeze.optimize.LBFGS: Step Size: 1.539\\n22/05/30 20:06:12 INFO breeze.optimize.LBFGS: Val and Grad Norm: 1.59952 (rel: 0.0198) 0.315022\\n22/05/30 20:06:12 INFO breeze.optimize.LBFGS: Converged because max iterations reached\\n22/05/30 20:06:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://plane-pyspark-run/Spark_Models/lr_model_all/metadata/' directory.\\n22/05/30 20:06:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://plane-pyspark-run/Spark_Models/lr_model_all/data/' directory.\\nTrain Accuracy : 0.29971906253475294\\nTest Accuracy : 0.2992301213537527\\n[[1.19183e+05 1.00331e+05 4.33900e+03 1.32670e+04 0.00000e+00 0.00000e+00\\n  0.00000e+00]\\n [5.87190e+04 1.07720e+05 1.32400e+03 8.39400e+03 0.00000e+00 0.00000e+00\\n  0.00000e+00]\\n [8.61830e+04 7.09640e+04 5.83700e+03 1.13540e+04 0.00000e+00 0.00000e+00\\n  0.00000e+00]\\n [6.04070e+04 7.36390e+04 1.87400e+03 1.95080e+04 0.00000e+00 0.00000e+00\\n  0.00000e+00]\\n [3.80590e+04 3.79510e+04 2.07100e+03 7.05600e+03 0.00000e+00 0.00000e+00\\n  0.00000e+00]\\n [4.51000e+03 6.23900e+03 4.60000e+01 1.80900e+03 0.00000e+00 0.00000e+00\\n  0.00000e+00]\\n [8.88000e+02 1.15400e+03 1.50000e+01 1.49000e+02 0.00000e+00 0.00000e+00\\n  0.00000e+00]]\\nAccuracy: 0.29971906253475294\\nFPR: 0.22073521672790777\\nTPR: 0.2997190625347529\\nF-measure: 0.23558397089322905\\nPrecision: 0.28475273254843625\\nRecall: 0.2997190625347529\\n22/05/30 20:22:50 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@5e961ee6{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\\n\"\n",
      "\n",
      "\n",
      "Model: Deleting PySpark Cluster.\n",
      "Cluster dse-230-pyspark successfully deleted.\n"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "948b3ef3c7f3779da0fb6aa870fa66a0cb8fafc1112b0f4c52e6a358a38e2d8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pyspark_230_env')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}